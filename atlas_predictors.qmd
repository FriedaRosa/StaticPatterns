---
title: "Atlas variables"
---

# Introduction

This code processes and analyzes spatial and species data related to atlas predictors and temporal change reponses.

-   It loads necessary libraries,
-   defines key variables,
-   reads in data,
-   performs calculations, and
-   produces summary statistics.

## Atlas Variables (Responses + Some Predictors)

### Libraries

```{r}
#| message: false
#| warning: false
#| error: false
#| output: false

# Load required packages
package_list <- c("here", "tidyverse", "tidyr", "dplyr", "sf", "skimr", "kableExtra")
invisible(lapply(package_list, library, character.only = TRUE))

# Enable s2 processing for sf package with projected data
sf_use_s2(TRUE)

# Clean environment and run garbage collection
rm(list = ls())
gc()
```

### Data paths

```{r}
# Define file paths for raw data and documentation
data_paths <- list(
  Documentation = "c:/Users/wolke/OneDrive - CZU v Praze/Dokumenty/Documents/GitHub/Project-folders/StaticPatterns/Documentation/",
  data_sf = "c:/Users/wolke/OneDrive - CZU v Praze/Dokumenty/Documents/GitHub/Project-folders/StaticPatterns/Data/output/1_data/1_data_sf.rds",
  data_filt = "c:/Users/wolke/OneDrive - CZU v Praze/Dokumenty/Documents/GitHub/Project-folders/StaticPatterns/Data/output/1_data/1_data_filtered.rds", 
  
  desired_levels = factor(c("1", "2", "4", "8", "16", "32", "64", "128"),
    ordered = T,
    levels = c("1", "2", "4", "8", "16", "32", "64", "128")),
    atlas_names = setNames(
    factor(c(5, 6, 13, 26)),
    c("Czechia", "NewYork", "Japan", "Europe")
  ),
  time_periods = c(1, 2),
  crs = c(
    "Czechia" = "epsg:5514",
    "NewYork" = "epsg:32118",
    "Japan" = "epsg:6684",
    "Europe" = "epsg:3035"
  )
  
)
```

## Read Spatial Data

(This dataset contains non-filtered data, including columns for filtering.)

```{r}
#| message: false

# Read spatial dataset
data_sf <- readRDS(data_paths$data_sf)
```

# Calculate Atlas Information

## Function to Calculate Grid Information

This function calculates key statistics from the spatial data, such as the total number of grid cells and sampled areas.

```{r}
calculate_grid_info <- function(data_sf) {
  grid_info <- full_join(
    data_sf %>%
      st_drop_geometry() %>%
      distinct(datasetID, scalingID, siteID, cell_sampling_repeats, croppedArea) %>%
      group_by(datasetID, scalingID) %>%
      summarise(Total_Ncells = n_distinct(siteID)),
    data_sf %>%
      st_drop_geometry() %>%
      distinct(datasetID, scalingID, siteID, cell_sampling_repeats, croppedArea) %>%
      filter(cell_sampling_repeats == 2) %>%
      group_by(datasetID, scalingID) %>%
      summarise(Total_Ncells_samp = n_distinct(siteID), Total_area_samp = sum(croppedArea, na.rm = TRUE))
  )
  return(grid_info)
}
```

## Calculate Atlas Information

```{r}
# Apply function to compute grid information
atlas_areas <- calculate_grid_info(data_sf)

# Overview
atlas_areas %>% kable()
# Summary statistics
atlas_areas %>% skim()

# Save results
write.csv(atlas_areas, paste0(data_paths$Documentation, "atlas_areas_METADATA.csv"))
```

## Load Filtered Species Data

```{r}
# Read processed species data
pres_dat_final <- readRDS(data_paths$data_filt) %>%
  left_join(atlas_areas) %>%
  mutate(scalingID = factor(scalingID, levels = data_paths$desired_levels)) %>%
  filter(!is.na(verbatimIdentification)) # Remove unsampled cells

head(pres_dat_final)
```

# Calculate Species Variables

## Jaccard Index Calculation

The Jaccard Index measures similarity between two sets.

```{r}
# Define Jaccard function
jaccard <- function(set1, set2) {
  a <- length(intersect(set1, set2))
  b <- length(setdiff(set2, set1))
  c <- length(setdiff(set1, set2))
  return(list(jaccard_index = a / (a + b + c), a = a, b = b, c = c))
}
```

### Calculate Jaccard Index for Each Dataset

```{r}
Jaccard_df <- pres_dat_final %>%
  filter(scalingID == 1) %>%
  group_by(datasetID, verbatimIdentification) %>%
  mutate(
    jaccard_results = list(jaccard(
      filter(pick(everything()), samplingPeriodID == 1)$siteID,
      filter(pick(everything()), samplingPeriodID == 2)$siteID
    ))
  ) %>%
  unnest_wider(jaccard_results) %>%
  rename(Jaccard_sim = jaccard_index) %>%
  mutate(Jaccard_dissim = 1 - Jaccard_sim) %>%
  ungroup() %>%
  distinct(datasetID, verbatimIdentification, Jaccard_dissim, Jaccard_sim, a, b, c)
```

### Inspect Jaccard

```{r}
# Checks:
head(Jaccard_df)
skim(Jaccard_df)
Jaccard_df %>% glimpse()
write.csv(Jaccard_df, paste0(data_paths$Documentation, "Jaccard_df.csv"))
```

### Merge with data

```{r}
# Merge:
pres_dat_final_v2 <- pres_dat_final %>%
  left_join(Jaccard_df)

head(pres_dat_final_v2)

rm(pres_dat_final, Jaccard_df, jaccard, atlas_areas, calculate_grid_info)
```

## Area of occupancy (AOO)

```{r}
# Calculate Occupancy across sampled area (croppedArea)

occ_data_final <- pres_dat_final_v2 %>%
  ungroup() %>%
  distinct(datasetID, samplingPeriodID, scalingID,
    verbatimIdentification, siteID,
    .keep_all = TRUE
  ) %>%
  group_by(datasetID, samplingPeriodID, scalingID, verbatimIdentification) %>%
  # Calculate AOO:
  dplyr::summarise(
    mean_area = mean(croppedArea, na.rm = TRUE),
    AOO = sum(croppedArea, na.rm = TRUE),
    occ_Ncells = n_distinct(siteID)
  ) %>%
  left_join(pres_dat_final_v2 %>%
    mutate(scalingID = as.factor(as.character(scalingID)))) %>%
  # Calculate relative Occupancy:
  dplyr::mutate(
    rel_AOO = AOO / Total_area_samp,
    rel_occ_Ncells = occ_Ncells / Total_Ncells_samp
  ) %>% # Prevalence

  # Remove duplicated rows:
  distinct() %>%
  select(-introduced)

saveRDS(occ_data_final, here::here("Data", "output", "1_data", "1_occ_data_final.rds"))
rm(pres_dat_final_v2)
```

### AOO checks:

```{r}
# Checks
colSums(is.na(occ_data_final)) # no NAs

head(occ_data_final)
round(range(occ_data_final$rel_AOO), 2)
round(range(occ_data_final$rel_occ_Ncells), 2)

plot(hist(occ_data_final$rel_occ_Ncells))
plot(hist(occ_data_final$rel_AOO))
plot(rel_AOO ~ rel_occ_Ncells,
  data = occ_data_final %>%
    select(samplingPeriodID, rel_AOO, rel_occ_Ncells, datasetID, verbatimIdentification, scalingID) %>%
    filter(scalingID == 1) %>%
    unique()
)

skim(occ_data_final %>% filter(scalingID == 1) %>% ungroup())
```

## Create species-level data

```{r}
species_data <- occ_data_final %>%
  dplyr::select(
    -siteID, -cell_sampling_repeats, -croppedArea, -sp_sampling_repeats
  ) %>%
  distinct(datasetID, samplingPeriodID, scalingID, verbatimIdentification,
    .keep_all = TRUE
  )

head(species_data)
names(species_data)

```

## Occupancy-Area-Relationship

### OAR-function:

```{r}
#| code-fold: true
# Function to calculate OAR:
library(dplyr)
library(tidyr)
library(purrr)
library(broom)

calculate_OAR <- function(species_data) {
  datasets <- unique(species_data$datasetID)
  sampling_periods <- unique(species_data$samplingPeriodID)
  
  sp_dta <- species_data %>%
    select(datasetID, samplingPeriodID, scalingID, 
           verbatimIdentification, 
           rel_AOO, rel_occ_Ncells, mean_area, AOO)

  # Expand the grid for all combinations of datasetID and samplingPeriodID
  species_data_new <- expand_grid(
    datasetID = datasets,
    samplingPeriodID = sampling_periods
  ) %>%
    inner_join(sp_dta, by = c("datasetID", "samplingPeriodID")) %>%
    # Group by identifiers
    group_by(datasetID, samplingPeriodID, verbatimIdentification) %>%
    # Get available scales where relative occupancy is not saturated (< 1)
    summarise(
      available_scales = n(),
      mean_relAOO = mean(rel_AOO, na.rm = TRUE),
      exclude_sp_OAR = if_else(available_scales < 2, 1, 0),
      .groups = "drop"
    ) %>%
    # remove those where the range is saturated at the smallest resolution
    mutate(
      exclude_sp_OAR = if_else(available_scales == 0, 1, exclude_sp_OAR),
      mean_relAOO = if_else(available_scales == 0, 1, mean_relAOO)
    ) %>%
    full_join(sp_dta, by = c("datasetID", "samplingPeriodID", "verbatimIdentification")) %>%
    filter(
      exclude_sp_OAR == 0,
      rel_occ_Ncells < 1
    ) %>%
    distinct() %>%
    filter_at(vars(scalingID, AOO, mean_area), any_vars(!is.na(.))) %>%
    ungroup()

  # Fit models using purrr::map
  species_data_new_v2 <- species_data_new %>%
    group_by(datasetID, samplingPeriodID, verbatimIdentification) %>%
    nest(data = c(scalingID, AOO, mean_area, rel_AOO, rel_occ_Ncells)) %>%
    mutate(
      coefficients = map(
        data,
        ~ .x %>%
          filter(
            !is.na(AOO) & AOO > 0,
            !is.na(mean_area) & mean_area > 0
          ) %>%
          lm(log(AOO) ~ log(mean_area), data = .) %>%
          coef() %>%
          {
            tibble(
              m_AOO_a = .[2],
              b_AOO_a = .[1],
              D_AOO_a = -2 * .[2] + 2
            )
          }
      )
    ) %>%
    unnest(coefficients) %>%
    ungroup() %>%
    # Final cleanup of results
    select(datasetID, samplingPeriodID, verbatimIdentification, m_AOO_a, b_AOO_a, D_AOO_a) %>%
    distinct() %>%
    # Merge with the original dataset
    full_join(species_data) %>%
    filter(scalingID == 1) %>%
    select(
      datasetID, verbatimIdentification, samplingPeriodID, Total_area_samp,
      Total_Ncells, Total_Ncells_samp, AOO, occ_Ncells, rel_occ_Ncells,
      rel_AOO, Jaccard_dissim, m_AOO_a, b_AOO_a, D_AOO_a
    )

  return(species_data_new_v2)
}
```

-   filtering steps below: -- drop species with \> 99% relative occupied area

```{r}
# Calculate OAR

species_data_new <- calculate_OAR(species_data) %>%
  distinct(datasetID, samplingPeriodID, verbatimIdentification, .keep_all = TRUE)

skim(species_data_new)
```

### Insepct OAR/Fractal results

```{r}
colSums(is.na(species_data_new))

species_data_new %>%
  filter(is.na(D_AOO_a)) # they all occupy > 99% of area (rel_occu > 0.99)
# We will assign them D = 2

species_data_new <- species_data_new %>% 
  mutate(
    D_AOO_a = case_when(is.na(D_AOO_a) ~ 2,
                        .default = D_AOO_a)
  )


rm(calculate_OAR)


species_data_new_v2 <- species_data_new %>%
  #na.omit() %>% # drop NAs in Occupancy-Area-relationship (45 rows)
  # Merge the results back with the original data
  left_join(species_data %>% filter(scalingID == 1)) %>%
  # reduce columns
  select(
    datasetID, verbatimIdentification, samplingPeriodID,
    Total_area_samp, Total_Ncells, Total_Ncells_samp,
    AOO, occ_Ncells, rel_occ_Ncells, rel_AOO,
    Jaccard_dissim, D_AOO_a
  ) %>%
  distinct(datasetID, samplingPeriodID, verbatimIdentification, .keep_all = TRUE)


colSums(is.na(species_data_new_v2)) # no more NAs


## Number of species with Fractal Dimension data:
species_data_new_v2 %>%
  group_by(datasetID) %>%
  summarize(n_sp = n_distinct(verbatimIdentification))

rm(species_data_new, species_data)

```

## Log ratio AOO

```{r}
# Create wide-data
library(tidyr)
time_periods <- data_paths$time_periods
```

```{r}
#| code-fold: true
transform_to_wide <- function(species_data_new, time_periods = c(1, 2)) {
  # Create a list to store wide data for each time period
  wide_dfs <- list()

  for (i in seq_along(time_periods)) {
    wide_dfs[[i]] <- species_data_new %>%
      distinct(datasetID, samplingPeriodID, verbatimIdentification, AOO) %>%
      group_by(datasetID, samplingPeriodID, verbatimIdentification) %>%
      filter(samplingPeriodID == time_periods[i]) %>%
      setNames(paste0("samplingPeriodID", i, "_", names(.))) %>%
      ungroup() %>%
      select(-c(paste0("samplingPeriodID", i, "_samplingPeriodID"))) %>%
      dplyr::rename(
        verbatimIdentification = paste0("samplingPeriodID", i, "_verbatimIdentification"),
        datasetID = paste0("samplingPeriodID", i, "_datasetID")
      )
  }

  # Merge the wide data frames sequentially
  sp_dat_wide <- reduce(wide_dfs, full_join, by = c("verbatimIdentification", "datasetID"))

  cat("NA counts in wide data after processing:\n")
  print(colSums(is.na(sp_dat_wide)))

  cat("Preview of wide data:\n")
  print(head(sp_dat_wide))

  return(sp_dat_wide)
}
```

```{r}
sp_dat_wide <- transform_to_wide(species_data_new_v2, time_periods) %>% na.omit() # drop species lost or gained completely

sp_dat_wide %>%
  group_by(datasetID) %>%
  summarise(n_sp = n_distinct(verbatimIdentification))
# final species numbers I guess?
```

## Log Ratio AOO

```{r}
logRatio <- sp_dat_wide %>%
  mutate(
    log_R2_1 = log(samplingPeriodID2_AOO / samplingPeriodID1_AOO),
    ratio_R2_1 = samplingPeriodID2_AOO / samplingPeriodID1_AOO) %>%
  select(-samplingPeriodID1_AOO, -samplingPeriodID2_AOO)

big_table <- full_join(species_data_new_v2, logRatio) %>%
  distinct(datasetID, samplingPeriodID, verbatimIdentification, 
           .keep_all = T) %>%
  mutate_if(is.numeric, round, 3)
```

### checks

```{r}
# check NAs
big_table %>%
  group_by(datasetID, samplingPeriodID) %>%
  dplyr::summarise(across(everything(), ~ sum(is.na(.x))))

head(big_table)
big_table %>% filter(is.na(log_R2_1)) # same species as before: those lost/gained completely I guess. we can drop them.

big_table %>% group_by(datasetID, samplingPeriodID) %>% skim()

big_table <- big_table %>% na.omit()

rm(logRatio, sp_dat_wide, species_data)
```

### Save

```{r}
# Save results
saveRDS(species_data_new_v2, here("Data/output/1_data/1_species_data.rds"))
saveRDS(big_table, here("Data/output/1_data/2_big_table.rds"))
```

## quick model summarys

```{r}
summary(lm(Jaccard_dissim ~ rel_occ_Ncells * datasetID + D_AOO_a,
           data = big_table %>%
             filter(samplingPeriodID == 1)
))

ranger::ranger(Jaccard_dissim ~ rel_occ_Ncells + datasetID + D_AOO_a,
               data = big_table %>%
                 filter(samplingPeriodID == 1), mtry = 1, trees = 500
)
```

```{r}
sessionInfo()
```
